
### Метод опорных векторов

Смысл метода заключается в посторении особой разделяющей гиперплоскости. В общем случае, если выборка линейно разделима, то таких разделяющих плотностей будет множество. Идея метода в том, чтобы разумно распорядится этой свободой: выбрать плоскоскость которая максимально удалена от всех точек выборки - такое требование появилось из соображений, что большой "зазор" будет давать более устойчевые решения.

Все подходящие гиперплоскости образуют *полосу* которая ограничивается двумя гиперплоскостями и та гиперплоскость, которая посередине, будет решением. Соответсивенно, чтобы найти гиперплоскость с максимальным *зазором* нужно найти максимальную по ширение полосу разделяющих гиперплоскостей для классов.

Пусть x− и x+ — два обучающих объекта классов −1 и +1 соответственно, лежащие на границе полосы. Тогда ширина полосы есть
![ХДЕ?????](lin_type_1.png?raw=true "Optional Title")

Тогда, в случае линейно разделимой выборки получим 
![ХДЕ?????](lin_type_2.png?raw=true "Optional Title")

На практике классы редко линейно разделимы, тогда обобщим уравнение добавив штраф за ошибку. 
![ХДЕ?????](lin_type_3.png?raw=true "Optional Title")

***С*** константа управляющая компромисом между шириной полосы и точностью классификации.

Построим двойственную задачу. Для этого запишем уравнение Лагранжа для этой системы:
![ХДЕ?????](lin_type_4.png?raw=true "Optional Title")

И саму двойственную задачу:
![ХДЕ?????](lin_type_5.png?raw=true "Optional Title")
![ХДЕ?????](lin_type_6.png?raw=true "Optional Title")

Соответсвенно все объекты выборки можно разделить по отступу на три категории:

    1. Отступ больше либо равен нулю - тогда такие объекты классифицируются правильно и не влияют на классификацию (неинформативные).
    2. Отступ равен единице - такие объекты классифицируются правильно и лежат в точности на границе разделяющей полосы (опорные граничные).
    3. Отступ меньше единицы - такие лежат внутри полосы (опорные нарушители).

Уберая все ненужные переменные которые обнуляются получим задачу:
![ХДЕ?????](lin_type_7.png?raw=true "Optional Title")

В итоге алгоритм классификации сводится к:
![ХДЕ?????](lin_type_8.png?raw=true "Optional Title")
Порог - это любой вектор из граничных точек.

Ещё один способ классификации, это замена скалярного произведения на функцию Ядра. Таку. функцию можно взять, или построить конструктивно:
![ХДЕ?????](lin_type_9.png?raw=true "Optional Title")

И наш алгоритм классификации преобразуется следующим образом:
![ХДЕ?????](lin_type_10.png?raw=true "Optional Title")

Можно представить, как двуслойную нейронную сеть:

***Преимущества SVM***

    1. Максимизация *зазора* учулшает обобщающую способность.
    2. Число нейронов определяется автоматически - равно числу опорных векторов.
    3. Если сводится к задаче квадратичного программирования, то имеет одно решение.
    
***Недостатки SVM***

    1. Неустойчивость к шуму. Объекты выброс будут считаться опорными.
    2. Нету общих методов подбора ядер для конкретной задачи.
    3. Подбор параметра ***С*** требует многократного решения задачи.
